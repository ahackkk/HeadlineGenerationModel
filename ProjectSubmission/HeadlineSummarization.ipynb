{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OZWudRmHJWJG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/kg_dataset (1).csv\")"
      ],
      "metadata": {
        "id": "Quq2MlPhJZOo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna().drop_duplicates()"
      ],
      "metadata": {
        "id": "qdrEUhMMJlqj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  #many spaces\n",
        "    text = text.strip()               #spaces at start and end\n",
        "    return text\n",
        "\n",
        "df[\"Text\"] = df[\"Text\"].apply(clean_text)\n",
        "df[\"headline\"] = df[\"headline\"].apply(clean_text)"
      ],
      "metadata": {
        "id": "V7J8GNzHMA_Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512\n",
        "df = df[df[\"Text\"].str.len() < MAX_LEN]"
      ],
      "metadata": {
        "id": "NafRX1eKMwbv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"Text\"].tolist(),\n",
        "    df[\"headline\"].tolist(),\n",
        "    test_size=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_texts))\n",
        "print(\"Validation size:\", len(val_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOvc-p9UqS9A",
        "outputId": "1a02a00f-e7a5-491a-b14c-78bfecd64be5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 1555\n",
            "Validation size: 173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
        "import torch\n",
        "\n",
        "#model\n",
        "model_name = \"google/mt5-small\"\n",
        "\n",
        "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLZqFLpGsLJ7",
        "outputId": "b5231a01-e2cf-43ee-c3d1-c945fb013b42"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
            "The class this function is called from is 'MT5Tokenizer'.\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#max input characters and max output\n",
        "MAX_INPUT_LENGTH = 512\n",
        "MAX_TARGET_LENGTH = 52\n",
        "\n",
        "#training tokenization\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=MAX_INPUT_LENGTH)\n",
        "train_targets = tokenizer(train_labels, truncation=True, padding=True, max_length=MAX_TARGET_LENGTH)\n",
        "\n",
        "print(train_texts[:5])\n",
        "print(train_labels[:5])\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"LABEL\", i, \":\", tokenizer.decode(train_targets[\"input_ids\"][i]))\n",
        "\n",
        "#validation tokenization\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=MAX_INPUT_LENGTH)\n",
        "val_targets = tokenizer(val_labels, truncation=True, padding=True, max_length=MAX_TARGET_LENGTH)"
      ],
      "metadata": {
        "id": "iypwAuirsWJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d22aa62-4d15-4828-849c-8249411e91a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Бүгүн,5-декабрда Жогорку КеңешРавшанбек Сабировдуэмгек, социалдык коргоо жана миграция министри кызматына дайындоого макулдук берген. Депутаттар анын талапкерлигине талкуусуз добуш беришти. 77 депутат «макул» деп добуш берди. Жалпы 83 депутат катталган. Кечээ, 4-декабрдаЖылдыз Полотоваэмгек, социалдык камсыздоо жана миграция министри кызматынан бошотулду.Равшанбек Сабировэмгек, социалдык камсыздоо жана миграция министринин милдетин аткаруучу болуп дайындалган.', 'Билим берүү жана илим министрлиги мектептер үчүн мамлекеттик электрондук күндөлүктү түзүү иштери аяктап калганын билдирди. Министрлик тарабынан иштелип чыккан электрондук күндөлүк окуучулар жана алардын ата-энелери, мектептер үчүн акысыз болот.', 'Бүгүн,27-августта окумуштуу, академикИлгиз Төрөкулович Айтматовдүйнөдөн кайтты. Бул тууралуу анын кызыЖамиля Айтматовабилдирди. Илгиз Айтматов— улуу жазуучуЧыңгыз Айтматовдуниниси. Ал 1931-жылы 8-февралда Фрунзе шаарында туулган. Көп жыл бою Кыргыз улуттук академиясында эмгектенген. СЕРЕП МЕДИАмаалымат агенттигиИлгиз Айтматовдунүй-бүлөсүнө жана жакындарына терең кайгыруу менен көңүл айтат.', 'Кыргызстандык эки суучул Катардын борбору Доха шаарында өтө турган дүйнө чемпионаты катышат. Бул тууралуу Дене тарбия жана спорт департаментинен билдиришти. 2-18-февраль күндөрү өтө турган дүйнө биринчилигине Кыргызстандан Денис Петрашов менен Елизавета Печерских барат. Курама команда мелдешке башкы машыктыруучу Евгений Петрашовдун жетекчилиги алдында катышат. Дүйнө биринчилигинде жалпы 76 комплект медаль ойнотулат.', 'Кечээ, 4-ноябрда социалдык тармактарда «Бишкекте чет өлкөлүктөрдүн арасында мушташ катталды» деген видео тараган. Бишкек ШИИБден окуя боюнча маалымат беришти. Маалыматка караганда, мушташ тууралуу маалымат милицияга 3-ноябрда саат 04:50 чамасында түшкөн. Жерине Бишкек ШИИБдин кызматкерлери барган учурда пикир келишпестиктен улам мушташ чыкканы маалым болгон. Аталган факты боюнча териштирүү иштери башталган. Кылмышка шектүү катары 2001-жылы туулган эки чет өлкөлүк жаран кармалган. Тергөө уланууда.']\n",
            "['Депутаттар эмгек министрине Равшанбек Сабировдун талапкерлигин жактырды', 'Билим берүү министрлиги эми мектептерде электрондук күндөлүк колдонуларын айтты', 'Окумуштуу, академик Илгиз Айтматов дүйнө салды', 'Кыргызстандык эки суучул дүйнө чемпионатында күчүн сынайт', 'Бишкекте чет өлкөлүктөр арасында мушташ катталды. Милиция эмне дейт?']\n",
            "LABEL 0 : Депутаттар эмгек министрине Равшанбек Сабировдун талапкерлигин жактырды</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "LABEL 1 : Билим берүү министрлиги эми мектептерде электрондук күндөлүк колдонуларын айтты</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "LABEL 2 : Окумуштуу, академик Илгиз Айтматов дүйнө салды</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "LABEL 3 : Кыргызстандык эки суучул дүйнө чемпионатында күчүн сынайт</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "LABEL 4 : Бишкекте чет өлкөлүктөр арасында мушташ катталды. Милиция эмне дейт?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KyrgyzHeadlineDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, targets):\n",
        "        self.encodings = encodings\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        labels = self.targets[\"input_ids\"][idx]\n",
        "        #all pads to -100\n",
        "        labels = [label if label != tokenizer.pad_token_id else -100 for label in labels]\n",
        "\n",
        "        item = {\n",
        "            \"input_ids\": torch.tensor(self.encodings[\"input_ids\"][idx]),\n",
        "            \"attention_mask\": torch.tensor(self.encodings[\"attention_mask\"][idx]),\n",
        "            \"labels\": torch.tensor(labels),\n",
        "        }\n",
        "        return item"
      ],
      "metadata": {
        "id": "u4iaxq3QtPAI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = KyrgyzHeadlineDataset(train_encodings, train_targets)\n",
        "val_dataset = KyrgyzHeadlineDataset(val_encodings, val_targets)"
      ],
      "metadata": {
        "id": "7lLnqMwF0MXr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_steps=100,\n",
        "    do_eval=True,\n",
        "    save_total_limit=2,\n",
        "    predict_with_generate=True,\n",
        ")"
      ],
      "metadata": {
        "id": "PkSIloSDtVj0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "72ko63wVrFuo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGr9lZqmNoTr",
        "outputId": "c29aee77-f653-4dbd-a768-70ea70f0944f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "rouge_metric = load_metric(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge_metric.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels,\n",
        "        use_stemmer=True\n",
        "    )\n",
        "    return {\n",
        "        \"rouge1\": result[\"rouge1\"].mid.fmeasure,\n",
        "        \"rouge2\": result[\"rouge2\"].mid.fmeasure,\n",
        "        \"rougeL\": result[\"rougeL\"].mid.fmeasure,\n",
        "    }"
      ],
      "metadata": {
        "id": "N1BdacYcJ6E8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75fafa88-2dd3-4cf8-ec5a-581ea2759e38"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-093e4c08b2a2>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge_metric = load_metric(\"rouge\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-0QPA_uMOZU",
        "outputId": "28d16696-4ecc-4770-9ae7-d64da872c9dc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('./logs', ignore_errors=True)"
      ],
      "metadata": {
        "id": "UzdwwX2jrgxY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = train_dataset[0]\n",
        "print(sample['labels'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru5bk8MUsJbQ",
        "outputId": "04c6a34a-2ae4-4af4-ba04-28cdab3745a6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  1580, 152136,   3591,  10185, 101583,  33385,  13390, 166969,  33197,\n",
            "         11840,   8662,  40118, 170087,  39439,   1625,  76745,    686,  30513,\n",
            "        186153,      1,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"input_ids:\", tokenizer.decode(sample[\"input_ids\"]))\n",
        "print(\"labels   :\", tokenizer.decode([token if token != -100 else tokenizer.pad_token_id for token in sample[\"labels\"]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJFejpRStYc-",
        "outputId": "5f10efb4-cf32-47e6-929f-86b995eb3c71"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids: Бүгүн,5-декабрда Жогорку КеңешРавшанбек Сабировдуэмгек, социалдык коргоо жана миграция министри кызматына дайындоого макулдук берген. Депутаттар анын талапкерлигине талкуусуз добуш беришти. 77 депутат «макул» деп добуш берди. Жалпы 83 депутат катталган. Кечээ, 4-декабрдаЖылдыз Полотоваэмгек, социалдык камсыздоо жана миграция министри кызматынан бошотулду.Равшанбек Сабировэмгек, социалдык камсыздоо жана миграция министринин милдетин аткаруучу болуп дайындалган.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "labels   : Депутаттар эмгек министрине Равшанбек Сабировдун талапкерлигин жактырды</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_for_decode = [token if token != -100 else tokenizer.pad_token_id for token in sample[\"labels\"]]\n",
        "print(tokenizer.decode(labels_for_decode))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqaK-D12tLMn",
        "outputId": "aaf5b214-8f96-4891-d8b2-dde57841f6ea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Депутаттар эмгек министрине Равшанбек Сабировдун талапкерлигин жактырды</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train dataset size:\", len(train_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty338K3Er3Ps",
        "outputId": "0230baea-5a35-45eb-f948-da6c4e74112e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 1555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_steps=100,\n",
        "    do_eval=True,\n",
        "    save_total_limit=2,\n",
        "    predict_with_generate=True,\n",
        "    report_to=\"none\", #wandb off\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "-BoM2AYUua1K",
        "outputId": "6a86132d-f5ff-4bbe-bf23-476cf715753e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-227ada6fca7b>:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='585' max='585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [585/585 09:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>19.115800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>11.924400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>8.888400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>6.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>5.814400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>4.982300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>4.785400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>4.426900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>4.322000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>4.212200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>4.257300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=585, training_loss=7.059413055680756, metrics={'train_runtime': 596.3621, 'train_samples_per_second': 7.822, 'train_steps_per_second': 0.981, 'total_flos': 1088780296089600.0, 'train_loss': 7.059413055680756, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"headline_model\")\n",
        "tokenizer.save_pretrained(\"headline_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGtDHi5Ly8C0",
        "outputId": "f989f942-37d3-4305-c766-94a4d11cdb25"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('headline_model/tokenizer_config.json',\n",
              " 'headline_model/special_tokens_map.json',\n",
              " 'headline_model/spiece.model',\n",
              " 'headline_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
        "\n",
        "model = MT5ForConditionalGeneration.from_pretrained(\"headline_model\").to(\"cuda\")\n",
        "tokenizer = MT5Tokenizer.from_pretrained(\"headline_model\")\n",
        "\n",
        "text = \"Бүгүн эртең менен жаан жаап баштады, азыр жаабай калды, күн чыгып кайра ысык болуп калды\"\n",
        "\n",
        "# Указываем max_length явно\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
        "\n",
        "# Генерация заголовка\n",
        "summary = model.generate(\n",
        "    **inputs,\n",
        "    max_length=64,\n",
        "    num_beams=4,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(summary[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "9zqXCu3M02qL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352a0540-bdb2-4d9c-c788-431a9f4119a9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<extra_id_0> жаап баштады\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "kMp-uJVM2M4E",
        "outputId": "2b623204-cf66-4320-96cc-d0805fa0dcac"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [22/22 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 2.914217948913574, 'eval_runtime': 1.6819, 'eval_samples_per_second': 102.861, 'eval_steps_per_second': 13.081, 'epoch': 3.0}\n"
          ]
        }
      ]
    }
  ]
}