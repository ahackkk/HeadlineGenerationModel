{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OZWudRmHJWJG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/kg_dataset (1).csv\")"
      ],
      "metadata": {
        "id": "Quq2MlPhJZOo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna().drop_duplicates()"
      ],
      "metadata": {
        "id": "qdrEUhMMJlqj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  #many spaces\n",
        "    text = text.strip()               #spaces at start and end\n",
        "    return text\n",
        "\n",
        "df[\"Text\"] = df[\"Text\"].apply(clean_text)\n",
        "df[\"headline\"] = df[\"headline\"].apply(clean_text)"
      ],
      "metadata": {
        "id": "V7J8GNzHMA_Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512\n",
        "df = df[df[\"Text\"].str.len() < MAX_LEN]"
      ],
      "metadata": {
        "id": "NafRX1eKMwbv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"Text\"].tolist(),\n",
        "    df[\"headline\"].tolist(),\n",
        "    test_size=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_texts))\n",
        "print(\"Validation size:\", len(val_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOvc-p9UqS9A",
        "outputId": "1a02a00f-e7a5-491a-b14c-78bfecd64be5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 1555\n",
            "Validation size: 173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
        "import torch\n",
        "\n",
        "#model\n",
        "model_name = \"google/mt5-small\"\n",
        "\n",
        "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLZqFLpGsLJ7",
        "outputId": "b5231a01-e2cf-43ee-c3d1-c945fb013b42"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
            "The class this function is called from is 'MT5Tokenizer'.\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#max input characters and max output\n",
        "MAX_INPUT_LENGTH = 512\n",
        "MAX_TARGET_LENGTH = 52\n",
        "\n",
        "#training tokenization\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=MAX_INPUT_LENGTH)\n",
        "train_targets = tokenizer(train_labels, truncation=True, padding=True, max_length=MAX_TARGET_LENGTH)\n",
        "\n",
        "print(train_texts[:5])\n",
        "print(train_labels[:5])\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"LABEL\", i, \":\", tokenizer.decode(train_targets[\"input_ids\"][i]))\n",
        "\n",
        "#validation tokenization\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=MAX_INPUT_LENGTH)\n",
        "val_targets = tokenizer(val_labels, truncation=True, padding=True, max_length=MAX_TARGET_LENGTH)"
      ],
      "metadata": {
        "id": "iypwAuirsWJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d22aa62-4d15-4828-849c-8249411e91a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['–ë“Ø–≥“Ø–Ω,5-–¥–µ–∫–∞–±—Ä–¥–∞ –ñ–æ–≥–æ—Ä–∫—É –ö–µ“£–µ—à–†–∞–≤—à–∞–Ω–±–µ–∫ –°–∞–±–∏—Ä–æ–≤–¥—É—ç–º–≥–µ–∫, —Å–æ—Ü–∏–∞–ª–¥—ã–∫ –∫–æ—Ä–≥–æ–æ –∂–∞–Ω–∞ –º–∏–≥—Ä–∞—Ü–∏—è –º–∏–Ω–∏—Å—Ç—Ä–∏ –∫—ã–∑–º–∞—Ç—ã–Ω–∞ –¥–∞–π—ã–Ω–¥–æ–æ–≥–æ –º–∞–∫—É–ª–¥—É–∫ –±–µ—Ä–≥–µ–Ω. –î–µ–ø—É—Ç–∞—Ç—Ç–∞—Ä –∞–Ω—ã–Ω —Ç–∞–ª–∞–ø–∫–µ—Ä–ª–∏–≥–∏–Ω–µ —Ç–∞–ª–∫—É—É—Å—É–∑ –¥–æ–±—É—à –±–µ—Ä–∏—à—Ç–∏. 77 –¥–µ–ø—É—Ç–∞—Ç ¬´–º–∞–∫—É–ª¬ª –¥–µ–ø –¥–æ–±—É—à –±–µ—Ä–¥–∏. –ñ–∞–ª–ø—ã 83 –¥–µ–ø—É—Ç–∞—Ç –∫–∞—Ç—Ç–∞–ª–≥–∞–Ω. –ö–µ—á—ç—ç, 4-–¥–µ–∫–∞–±—Ä–¥–∞–ñ—ã–ª–¥—ã–∑ –ü–æ–ª–æ—Ç–æ–≤–∞—ç–º–≥–µ–∫, —Å–æ—Ü–∏–∞–ª–¥—ã–∫ –∫–∞–º—Å—ã–∑–¥–æ–æ –∂–∞–Ω–∞ –º–∏–≥—Ä–∞—Ü–∏—è –º–∏–Ω–∏—Å—Ç—Ä–∏ –∫—ã–∑–º–∞—Ç—ã–Ω–∞–Ω –±–æ—à–æ—Ç—É–ª–¥—É.–†–∞–≤—à–∞–Ω–±–µ–∫ –°–∞–±–∏—Ä–æ–≤—ç–º–≥–µ–∫, —Å–æ—Ü–∏–∞–ª–¥—ã–∫ –∫–∞–º—Å—ã–∑–¥–æ–æ –∂–∞–Ω–∞ –º–∏–≥—Ä–∞—Ü–∏—è –º–∏–Ω–∏—Å—Ç—Ä–∏–Ω–∏–Ω –º–∏–ª–¥–µ—Ç–∏–Ω –∞—Ç–∫–∞—Ä—É—É—á—É –±–æ–ª—É–ø –¥–∞–π—ã–Ω–¥–∞–ª–≥–∞–Ω.', '–ë–∏–ª–∏–º –±–µ—Ä“Ø“Ø –∂–∞–Ω–∞ –∏–ª–∏–º –º–∏–Ω–∏—Å—Ç—Ä–ª–∏–≥–∏ –º–µ–∫—Ç–µ–ø—Ç–µ—Ä “Ø—á“Ø–Ω –º–∞–º–ª–µ–∫–µ—Ç—Ç–∏–∫ —ç–ª–µ–∫—Ç—Ä–æ–Ω–¥—É–∫ –∫“Ø–Ω–¥”©–ª“Ø–∫—Ç“Ø —Ç“Ø–∑“Ø“Ø –∏—à—Ç–µ—Ä–∏ –∞—è–∫—Ç–∞–ø –∫–∞–ª–≥–∞–Ω—ã–Ω –±–∏–ª–¥–∏—Ä–¥–∏. –ú–∏–Ω–∏—Å—Ç—Ä–ª–∏–∫ —Ç–∞—Ä–∞–±—ã–Ω–∞–Ω –∏—à—Ç–µ–ª–∏–ø —á—ã–∫–∫–∞–Ω —ç–ª–µ–∫—Ç—Ä–æ–Ω–¥—É–∫ –∫“Ø–Ω–¥”©–ª“Ø–∫ –æ–∫—É—É—á—É–ª–∞—Ä –∂–∞–Ω–∞ –∞–ª–∞—Ä–¥—ã–Ω –∞—Ç–∞-—ç–Ω–µ–ª–µ—Ä–∏, –º–µ–∫—Ç–µ–ø—Ç–µ—Ä “Ø—á“Ø–Ω –∞–∫—ã—Å—ã–∑ –±–æ–ª–æ—Ç.', '–ë“Ø–≥“Ø–Ω,27-–∞–≤–≥—É—Å—Ç—Ç–∞ –æ–∫—É–º—É—à—Ç—É—É, –∞–∫–∞–¥–µ–º–∏–∫–ò–ª–≥–∏–∑ –¢”©—Ä”©–∫—É–ª–æ–≤–∏—á –ê–π—Ç–º–∞—Ç–æ–≤–¥“Ø–π–Ω”©–¥”©–Ω –∫–∞–π—Ç—Ç—ã. –ë—É–ª —Ç—É—É—Ä–∞–ª—É—É –∞–Ω—ã–Ω –∫—ã–∑—ã–ñ–∞–º–∏–ª—è –ê–π—Ç–º–∞—Ç–æ–≤–∞–±–∏–ª–¥–∏—Ä–¥–∏. –ò–ª–≥–∏–∑ –ê–π—Ç–º–∞—Ç–æ–≤‚Äî —É–ª—É—É –∂–∞–∑—É—É—á—É–ß—ã“£–≥—ã–∑ –ê–π—Ç–º–∞—Ç–æ–≤–¥—É–Ω–∏–Ω–∏—Å–∏. –ê–ª 1931-–∂—ã–ª—ã 8-—Ñ–µ–≤—Ä–∞–ª–¥–∞ –§—Ä—É–Ω–∑–µ —à–∞–∞—Ä—ã–Ω–¥–∞ —Ç—É—É–ª–≥–∞–Ω. –ö”©–ø –∂—ã–ª –±–æ—é –ö—ã—Ä–≥—ã–∑ —É–ª—É—Ç—Ç—É–∫ –∞–∫–∞–¥–µ–º–∏—è—Å—ã–Ω–¥–∞ —ç–º–≥–µ–∫—Ç–µ–Ω–≥–µ–Ω. –°–ï–†–ï–ü –ú–ï–î–ò–ê–º–∞–∞–ª—ã–º–∞—Ç –∞–≥–µ–Ω—Ç—Ç–∏–≥–∏–ò–ª–≥–∏–∑ –ê–π—Ç–º–∞—Ç–æ–≤–¥—É–Ω“Ø–π-–±“Ø–ª”©—Å“Ø–Ω”© –∂–∞–Ω–∞ –∂–∞–∫—ã–Ω–¥–∞—Ä—ã–Ω–∞ —Ç–µ—Ä–µ“£ –∫–∞–π–≥—ã—Ä—É—É –º–µ–Ω–µ–Ω –∫”©“£“Ø–ª –∞–π—Ç–∞—Ç.', '–ö—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω–¥—ã–∫ —ç–∫–∏ —Å—É—É—á—É–ª –ö–∞—Ç–∞—Ä–¥—ã–Ω –±–æ—Ä–±–æ—Ä—É –î–æ—Ö–∞ —à–∞–∞—Ä—ã–Ω–¥–∞ ”©—Ç”© —Ç—É—Ä–≥–∞–Ω –¥“Ø–π–Ω”© —á–µ–º–ø–∏–æ–Ω–∞—Ç—ã –∫–∞—Ç—ã—à–∞—Ç. –ë—É–ª —Ç—É—É—Ä–∞–ª—É—É –î–µ–Ω–µ —Ç–∞—Ä–±–∏—è –∂–∞–Ω–∞ —Å–ø–æ—Ä—Ç –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∏–Ω–µ–Ω –±–∏–ª–¥–∏—Ä–∏—à—Ç–∏. 2-18-—Ñ–µ–≤—Ä–∞–ª—å –∫“Ø–Ω–¥”©—Ä“Ø ”©—Ç”© —Ç—É—Ä–≥–∞–Ω –¥“Ø–π–Ω”© –±–∏—Ä–∏–Ω—á–∏–ª–∏–≥–∏–Ω–µ –ö—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω–¥–∞–Ω –î–µ–Ω–∏—Å –ü–µ—Ç—Ä–∞—à–æ–≤ –º–µ–Ω–µ–Ω –ï–ª–∏–∑–∞–≤–µ—Ç–∞ –ü–µ—á–µ—Ä—Å–∫–∏—Ö –±–∞—Ä–∞—Ç. –ö—É—Ä–∞–º–∞ –∫–æ–º–∞–Ω–¥–∞ –º–µ–ª–¥–µ—à–∫–µ –±–∞—à–∫—ã –º–∞—à—ã–∫—Ç—ã—Ä—É—É—á—É –ï–≤–≥–µ–Ω–∏–π –ü–µ—Ç—Ä–∞—à–æ–≤–¥—É–Ω –∂–µ—Ç–µ–∫—á–∏–ª–∏–≥–∏ –∞–ª–¥—ã–Ω–¥–∞ –∫–∞—Ç—ã—à–∞—Ç. –î“Ø–π–Ω”© –±–∏—Ä–∏–Ω—á–∏–ª–∏–≥–∏–Ω–¥–µ –∂–∞–ª–ø—ã 76 –∫–æ–º–ø–ª–µ–∫—Ç –º–µ–¥–∞–ª—å –æ–π–Ω–æ—Ç—É–ª–∞—Ç.', '–ö–µ—á—ç—ç, 4-–Ω–æ—è–±—Ä–¥–∞ —Å–æ—Ü–∏–∞–ª–¥—ã–∫ —Ç–∞—Ä–º–∞–∫—Ç–∞—Ä–¥–∞ ¬´–ë–∏—à–∫–µ–∫—Ç–µ —á–µ—Ç ”©–ª–∫”©–ª“Ø–∫—Ç”©—Ä–¥“Ø–Ω –∞—Ä–∞—Å—ã–Ω–¥–∞ –º—É—à—Ç–∞—à –∫–∞—Ç—Ç–∞–ª–¥—ã¬ª –¥–µ–≥–µ–Ω –≤–∏–¥–µ–æ —Ç–∞—Ä–∞–≥–∞–Ω. –ë–∏—à–∫–µ–∫ –®–ò–ò–ë–¥–µ–Ω –æ–∫—É—è –±–æ—é–Ω—á–∞ –º–∞–∞–ª—ã–º–∞—Ç –±–µ—Ä–∏—à—Ç–∏. –ú–∞–∞–ª—ã–º–∞—Ç–∫–∞ –∫–∞—Ä–∞–≥–∞–Ω–¥–∞, –º—É—à—Ç–∞—à —Ç—É—É—Ä–∞–ª—É—É –º–∞–∞–ª—ã–º–∞—Ç –º–∏–ª–∏—Ü–∏—è–≥–∞ 3-–Ω–æ—è–±—Ä–¥–∞ —Å–∞–∞—Ç 04:50 —á–∞–º–∞—Å—ã–Ω–¥–∞ —Ç“Ø—à–∫”©–Ω. –ñ–µ—Ä–∏–Ω–µ –ë–∏—à–∫–µ–∫ –®–ò–ò–ë–¥–∏–Ω –∫—ã–∑–º–∞—Ç–∫–µ—Ä–ª–µ—Ä–∏ –±–∞—Ä–≥–∞–Ω —É—á—É—Ä–¥–∞ –ø–∏–∫–∏—Ä –∫–µ–ª–∏—à–ø–µ—Å—Ç–∏–∫—Ç–µ–Ω —É–ª–∞–º –º—É—à—Ç–∞—à —á—ã–∫–∫–∞–Ω—ã –º–∞–∞–ª—ã–º –±–æ–ª–≥–æ–Ω. –ê—Ç–∞–ª–≥–∞–Ω —Ñ–∞–∫—Ç—ã –±–æ—é–Ω—á–∞ —Ç–µ—Ä–∏—à—Ç–∏—Ä“Ø“Ø –∏—à—Ç–µ—Ä–∏ –±–∞—à—Ç–∞–ª–≥–∞–Ω. –ö—ã–ª–º—ã—à–∫–∞ —à–µ–∫—Ç“Ø“Ø –∫–∞—Ç–∞—Ä—ã 2001-–∂—ã–ª—ã —Ç—É—É–ª–≥–∞–Ω —ç–∫–∏ —á–µ—Ç ”©–ª–∫”©–ª“Ø–∫ –∂–∞—Ä–∞–Ω –∫–∞—Ä–º–∞–ª–≥–∞–Ω. –¢–µ—Ä–≥”©”© —É–ª–∞–Ω—É—É–¥–∞.']\n",
            "['–î–µ–ø—É—Ç–∞—Ç—Ç–∞—Ä —ç–º–≥–µ–∫ –º–∏–Ω–∏—Å—Ç—Ä–∏–Ω–µ –†–∞–≤—à–∞–Ω–±–µ–∫ –°–∞–±–∏—Ä–æ–≤–¥—É–Ω —Ç–∞–ª–∞–ø–∫–µ—Ä–ª–∏–≥–∏–Ω –∂–∞–∫—Ç—ã—Ä–¥—ã', '–ë–∏–ª–∏–º –±–µ—Ä“Ø“Ø –º–∏–Ω–∏—Å—Ç—Ä–ª–∏–≥–∏ —ç–º–∏ –º–µ–∫—Ç–µ–ø—Ç–µ—Ä–¥–µ —ç–ª–µ–∫—Ç—Ä–æ–Ω–¥—É–∫ –∫“Ø–Ω–¥”©–ª“Ø–∫ –∫–æ–ª–¥–æ–Ω—É–ª–∞—Ä—ã–Ω –∞–π—Ç—Ç—ã', '–û–∫—É–º—É—à—Ç—É—É, –∞–∫–∞–¥–µ–º–∏–∫ –ò–ª–≥–∏–∑ –ê–π—Ç–º–∞—Ç–æ–≤ –¥“Ø–π–Ω”© —Å–∞–ª–¥—ã', '–ö—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω–¥—ã–∫ —ç–∫–∏ —Å—É—É—á—É–ª –¥“Ø–π–Ω”© —á–µ–º–ø–∏–æ–Ω–∞—Ç—ã–Ω–¥–∞ –∫“Ø—á“Ø–Ω —Å—ã–Ω–∞–π—Ç', '–ë–∏—à–∫–µ–∫—Ç–µ —á–µ—Ç ”©–ª–∫”©–ª“Ø–∫—Ç”©—Ä –∞—Ä–∞—Å—ã–Ω–¥–∞ –º—É—à—Ç–∞—à –∫–∞—Ç—Ç–∞–ª–¥—ã. –ú–∏–ª–∏—Ü–∏—è —ç–º–Ω–µ –¥–µ–π—Ç?']\n",
            "LABEL 0 : –î–µ–ø—É—Ç–∞—Ç—Ç–∞—Ä —ç–º–≥–µ–∫ –º–∏–Ω–∏—Å—Ç—Ä–∏–Ω–µ –†–∞–≤—à–∞–Ω–±–µ–∫ –°–∞–±–∏—Ä–æ–≤–¥—É–Ω —Ç–∞–ª–∞–ø–∫–µ—Ä–ª–∏–≥–∏–Ω –∂–∞–∫—Ç—ã—Ä–¥—ã</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "LABEL 1 : –ë–∏–ª–∏–º –±–µ—Ä“Ø“Ø –º–∏–Ω–∏—Å—Ç—Ä–ª–∏–≥–∏ —ç–º–∏ –º–µ–∫—Ç–µ–ø—Ç–µ—Ä–¥–µ —ç–ª–µ–∫—Ç—Ä–æ–Ω–¥—É–∫ –∫“Ø–Ω–¥”©–ª“Ø–∫ –∫–æ–ª–¥–æ–Ω—É–ª–∞—Ä—ã–Ω –∞–π—Ç—Ç—ã</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "LABEL 2 : –û–∫—É–º—É—à—Ç—É—É, –∞–∫–∞–¥–µ–º–∏–∫ –ò–ª–≥–∏–∑ –ê–π—Ç–º–∞—Ç–æ–≤ –¥“Ø–π–Ω”© —Å–∞–ª–¥—ã</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "LABEL 3 : –ö—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω–¥—ã–∫ —ç–∫–∏ —Å—É—É—á—É–ª –¥“Ø–π–Ω”© —á–µ–º–ø–∏–æ–Ω–∞—Ç—ã–Ω–¥–∞ –∫“Ø—á“Ø–Ω —Å—ã–Ω–∞–π—Ç</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "LABEL 4 : –ë–∏—à–∫–µ–∫—Ç–µ —á–µ—Ç ”©–ª–∫”©–ª“Ø–∫—Ç”©—Ä –∞—Ä–∞—Å—ã–Ω–¥–∞ –º—É—à—Ç–∞—à –∫–∞—Ç—Ç–∞–ª–¥—ã. –ú–∏–ª–∏—Ü–∏—è —ç–º–Ω–µ –¥–µ–π—Ç?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KyrgyzHeadlineDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, targets):\n",
        "        self.encodings = encodings\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        labels = self.targets[\"input_ids\"][idx]\n",
        "        #all pads to -100\n",
        "        labels = [label if label != tokenizer.pad_token_id else -100 for label in labels]\n",
        "\n",
        "        item = {\n",
        "            \"input_ids\": torch.tensor(self.encodings[\"input_ids\"][idx]),\n",
        "            \"attention_mask\": torch.tensor(self.encodings[\"attention_mask\"][idx]),\n",
        "            \"labels\": torch.tensor(labels),\n",
        "        }\n",
        "        return item"
      ],
      "metadata": {
        "id": "u4iaxq3QtPAI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = KyrgyzHeadlineDataset(train_encodings, train_targets)\n",
        "val_dataset = KyrgyzHeadlineDataset(val_encodings, val_targets)"
      ],
      "metadata": {
        "id": "7lLnqMwF0MXr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_steps=100,\n",
        "    do_eval=True,\n",
        "    save_total_limit=2,\n",
        "    predict_with_generate=True,\n",
        ")"
      ],
      "metadata": {
        "id": "PkSIloSDtVj0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "72ko63wVrFuo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGr9lZqmNoTr",
        "outputId": "c29aee77-f653-4dbd-a768-70ea70f0944f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "rouge_metric = load_metric(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge_metric.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels,\n",
        "        use_stemmer=True\n",
        "    )\n",
        "    return {\n",
        "        \"rouge1\": result[\"rouge1\"].mid.fmeasure,\n",
        "        \"rouge2\": result[\"rouge2\"].mid.fmeasure,\n",
        "        \"rougeL\": result[\"rougeL\"].mid.fmeasure,\n",
        "    }"
      ],
      "metadata": {
        "id": "N1BdacYcJ6E8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75fafa88-2dd3-4cf8-ec5a-581ea2759e38"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-093e4c08b2a2>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge_metric = load_metric(\"rouge\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-0QPA_uMOZU",
        "outputId": "28d16696-4ecc-4770-9ae7-d64da872c9dc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('./logs', ignore_errors=True)"
      ],
      "metadata": {
        "id": "UzdwwX2jrgxY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = train_dataset[0]\n",
        "print(sample['labels'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru5bk8MUsJbQ",
        "outputId": "04c6a34a-2ae4-4af4-ba04-28cdab3745a6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  1580, 152136,   3591,  10185, 101583,  33385,  13390, 166969,  33197,\n",
            "         11840,   8662,  40118, 170087,  39439,   1625,  76745,    686,  30513,\n",
            "        186153,      1,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"input_ids:\", tokenizer.decode(sample[\"input_ids\"]))\n",
        "print(\"labels   :\", tokenizer.decode([token if token != -100 else tokenizer.pad_token_id for token in sample[\"labels\"]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJFejpRStYc-",
        "outputId": "5f10efb4-cf32-47e6-929f-86b995eb3c71"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids: –ë“Ø–≥“Ø–Ω,5-–¥–µ–∫–∞–±—Ä–¥–∞ –ñ–æ–≥–æ—Ä–∫—É –ö–µ“£–µ—à–†–∞–≤—à–∞–Ω–±–µ–∫ –°–∞–±–∏—Ä–æ–≤–¥—É—ç–º–≥–µ–∫, —Å–æ—Ü–∏–∞–ª–¥—ã–∫ –∫–æ—Ä–≥–æ–æ –∂–∞–Ω–∞ –º–∏–≥—Ä–∞—Ü–∏—è –º–∏–Ω–∏—Å—Ç—Ä–∏ –∫—ã–∑–º–∞—Ç—ã–Ω–∞ –¥–∞–π—ã–Ω–¥–æ–æ–≥–æ –º–∞–∫—É–ª–¥—É–∫ –±–µ—Ä–≥–µ–Ω. –î–µ–ø—É—Ç–∞—Ç—Ç–∞—Ä –∞–Ω—ã–Ω —Ç–∞–ª–∞–ø–∫–µ—Ä–ª–∏–≥–∏–Ω–µ —Ç–∞–ª–∫—É—É—Å—É–∑ –¥–æ–±—É—à –±–µ—Ä–∏—à—Ç–∏. 77 –¥–µ–ø—É—Ç–∞—Ç ¬´–º–∞–∫—É–ª¬ª –¥–µ–ø –¥–æ–±—É—à –±–µ—Ä–¥–∏. –ñ–∞–ª–ø—ã 83 –¥–µ–ø—É—Ç–∞—Ç –∫–∞—Ç—Ç–∞–ª–≥–∞–Ω. –ö–µ—á—ç—ç, 4-–¥–µ–∫–∞–±—Ä–¥–∞–ñ—ã–ª–¥—ã–∑ –ü–æ–ª–æ—Ç–æ–≤–∞—ç–º–≥–µ–∫, —Å–æ—Ü–∏–∞–ª–¥—ã–∫ –∫–∞–º—Å—ã–∑–¥–æ–æ –∂–∞–Ω–∞ –º–∏–≥—Ä–∞—Ü–∏—è –º–∏–Ω–∏—Å—Ç—Ä–∏ –∫—ã–∑–º–∞—Ç—ã–Ω–∞–Ω –±–æ—à–æ—Ç—É–ª–¥—É.–†–∞–≤—à–∞–Ω–±–µ–∫ –°–∞–±–∏—Ä–æ–≤—ç–º–≥–µ–∫, —Å–æ—Ü–∏–∞–ª–¥—ã–∫ –∫–∞–º—Å—ã–∑–¥–æ–æ –∂–∞–Ω–∞ –º–∏–≥—Ä–∞—Ü–∏—è –º–∏–Ω–∏—Å—Ç—Ä–∏–Ω–∏–Ω –º–∏–ª–¥–µ—Ç–∏–Ω –∞—Ç–∫–∞—Ä—É—É—á—É –±–æ–ª—É–ø –¥–∞–π—ã–Ω–¥–∞–ª–≥–∞–Ω.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "labels   : –î–µ–ø—É—Ç–∞—Ç—Ç–∞—Ä —ç–º–≥–µ–∫ –º–∏–Ω–∏—Å—Ç—Ä–∏–Ω–µ –†–∞–≤—à–∞–Ω–±–µ–∫ –°–∞–±–∏—Ä–æ–≤–¥—É–Ω —Ç–∞–ª–∞–ø–∫–µ—Ä–ª–∏–≥–∏–Ω –∂–∞–∫—Ç—ã—Ä–¥—ã</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_for_decode = [token if token != -100 else tokenizer.pad_token_id for token in sample[\"labels\"]]\n",
        "print(tokenizer.decode(labels_for_decode))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqaK-D12tLMn",
        "outputId": "aaf5b214-8f96-4891-d8b2-dde57841f6ea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–î–µ–ø—É—Ç–∞—Ç—Ç–∞—Ä —ç–º–≥–µ–∫ –º–∏–Ω–∏—Å—Ç—Ä–∏–Ω–µ –†–∞–≤—à–∞–Ω–±–µ–∫ –°–∞–±–∏—Ä–æ–≤–¥—É–Ω —Ç–∞–ª–∞–ø–∫–µ—Ä–ª–∏–≥–∏–Ω –∂–∞–∫—Ç—ã—Ä–¥—ã</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train dataset size:\", len(train_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty338K3Er3Ps",
        "outputId": "0230baea-5a35-45eb-f948-da6c4e74112e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 1555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_steps=100,\n",
        "    do_eval=True,\n",
        "    save_total_limit=2,\n",
        "    predict_with_generate=True,\n",
        "    report_to=\"none\", #wandb off\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "-BoM2AYUua1K",
        "outputId": "6a86132d-f5ff-4bbe-bf23-476cf715753e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-227ada6fca7b>:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='585' max='585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [585/585 09:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>19.115800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>11.924400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>8.888400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>6.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>5.814400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>4.982300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>4.785400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>4.426900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>4.322000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>4.212200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>4.257300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=585, training_loss=7.059413055680756, metrics={'train_runtime': 596.3621, 'train_samples_per_second': 7.822, 'train_steps_per_second': 0.981, 'total_flos': 1088780296089600.0, 'train_loss': 7.059413055680756, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"headline_model\")\n",
        "tokenizer.save_pretrained(\"headline_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGtDHi5Ly8C0",
        "outputId": "f989f942-37d3-4305-c766-94a4d11cdb25"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('headline_model/tokenizer_config.json',\n",
              " 'headline_model/special_tokens_map.json',\n",
              " 'headline_model/spiece.model',\n",
              " 'headline_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
        "\n",
        "model = MT5ForConditionalGeneration.from_pretrained(\"headline_model\").to(\"cuda\")\n",
        "tokenizer = MT5Tokenizer.from_pretrained(\"headline_model\")\n",
        "\n",
        "text = \"–ë“Ø–≥“Ø–Ω —ç—Ä—Ç–µ“£ –º–µ–Ω–µ–Ω –∂–∞–∞–Ω –∂–∞–∞–ø –±–∞—à—Ç–∞–¥—ã, –∞–∑—ã—Ä –∂–∞–∞–±–∞–π –∫–∞–ª–¥—ã, –∫“Ø–Ω —á—ã–≥—ã–ø –∫–∞–π—Ä–∞ —ã—Å—ã–∫ –±–æ–ª—É–ø –∫–∞–ª–¥—ã\"\n",
        "\n",
        "# –£–∫–∞–∑—ã–≤–∞–µ–º max_length —è–≤–Ω–æ\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
        "\n",
        "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞\n",
        "summary = model.generate(\n",
        "    **inputs,\n",
        "    max_length=64,\n",
        "    num_beams=4,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(summary[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "9zqXCu3M02qL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352a0540-bdb2-4d9c-c788-431a9f4119a9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<extra_id_0> –∂–∞–∞–ø –±–∞—à—Ç–∞–¥—ã\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "kMp-uJVM2M4E",
        "outputId": "2b623204-cf66-4320-96cc-d0805fa0dcac"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [22/22 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 2.914217948913574, 'eval_runtime': 1.6819, 'eval_samples_per_second': 102.861, 'eval_steps_per_second': 13.081, 'epoch': 3.0}\n"
          ]
        }
      ]
    }
  ]
}